# Mini GPT Wikip√©dia

Petit projet **Personnel** pour construire un **mini GPT** entra√Æn√© sur un sous-ensemble de **Wikip√©diaüá´üá∑**, en 3 parties :

1. **Scraper** : collecte propre depuis l‚ÄôAPI Wikip√©dia, √† partir d‚Äôune page ¬´ m√®re ¬ª et en explorant ses liens (profondeur contr√¥l√©e).
2. **Entra√Ænement** : un **TinyGPT** (transformer causal) minimal, entra√Æn√© sur les textes tokenis√©s.
3. **Using** : une **app Streamlit** pour chatter avec le mod√®le.

> Le code d√©tecte automatiquement **CUDA ‚Üí MPS ‚Üí CPU**. Con√ßu pour tourner sur **Mac (Apple Silicon)** comme sur Linux/Windows.

---



## üñºÔ∏è Aper√ßu de l'interface


![Interface Streamlit](Image/Results.png)


---

## ‚öôÔ∏è Pr√©requis
- Python **3.10+**
- **PyTorch** 2.2+ (avec MPS sur Mac, ou CUDA si dispo)
- Acc√®s internet pour le scraping

---

## üöÄ Installation

```bash
# (Optionnel) environnement virtuel
python3 -m venv .venv && source .venv/bin/activate

# Installer PyTorch (voir la commande adapt√©e sur pytorch.org si GPU CUDA)
pip install torch torchvision torchaudio

# D√©pendances du projet
pip install -r requirements.txt
```

`requirements.txt` minimal :
```
wikipedia-api
mwparserfromhell
transformers>=4.41
torch>=2.2
numpy==1.26.4
tqdm
streamlit
```

---

## üóÇÔ∏è Arborescence
```
mini-gpt-wiki/
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                  
‚îÇ  ‚îî‚îÄ tokens/               
‚îú‚îÄ models/
‚îÇ  ‚îî‚îÄ tinygpt/              
‚îú‚îÄ scraper/
‚îÇ  ‚îî‚îÄ wiki_scraper.py       
‚îú‚îÄ training/
‚îÇ  ‚îú‚îÄ dataset.py           
‚îÇ  ‚îú‚îÄ tiny_gpt.py          
‚îÇ  ‚îú‚îÄ train.py               
‚îÇ  ‚îî‚îÄ generate.py           
‚îú‚îÄ app/
‚îÇ  ‚îî‚îÄ app_streamlit.py     
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md
```

---

## üß≠ Workflow 

### 1) Scraper Wikip√©dia
Choisir une **page m√®re**, la **langue**, la **profondeur** et un **max de pages** (exemple FR sur *Intelligence artificielle*).

```bash
python scraper/wiki_scraper.py "Intelligence artificielle" \
  --lang fr --depth 1 --max_pages 300 --sleep 0.2 \
  --user_agent "MiniGPTWiki/1.0 (prenom; prenom@exemple.com)" \
  --out data/raw/wiki_fr_ia.jsonl
```

> Le script affiche une barre de progression `Scraping pages` et cr√©e `data/raw/wiki_fr_ia.jsonl`.

### 2) Construire le dataset tokenis√©

```bash
python -c "from training.dataset import build_token_dataset; \
build_token_dataset('data/raw/wiki_fr_ia.jsonl','data/tokens/gpt2_fr_ia')"

ls -lh data/tokens/gpt2_fr_ia
cat data/tokens/gpt2_fr_ia/meta.json
```

### 3) Entra√Æner le mini-GPT

```bash
python training/train.py
```

- Le script choisit automatiquement le **device** (CUDA ‚Üí MPS ‚Üí CPU).
- Les checkpoints sont √©crits dans `models/tinygpt/` (ex. `tinygpt_final.pt`).

> **Preset POC (optionnel)** : dans `train.py`, √† la fin, tu peux remplacer l‚Äôappel par :
```python
train(max_steps=800, batch_size=12, d_model=320, n_layer=4, n_head=4, block_size=256)
```

### 4) G√©n√©rer en CLI (Utiliser le LLM dans le terminal)

```bash
python training/generate.py
```

> Modifie le prompt dans `generate.py` si besoin.

### 5) Lancer l‚Äôapp Streamlit (Utiliser le LLM sur une interface WEB)

```bash
streamlit run app/app_streamlit.py
```
Dans l‚ÄôUI :
- **Checkpoint** : `models/tinygpt/tinygpt_final.pt`
- **Tokens dir** : `data/tokens/gpt2_fr_ia`

---

## üß† D√©tails techniques

### D√©tection du device
Les scripts utilisent :
```python
if torch.cuda.is_available(): device = 'cuda'
elif torch.backends.mps.is_available(): device = 'mps'
else: device = 'cpu'
```

### Tokenizer
- Par d√©faut : **GPT‚Äë2 byte-level** via `transformers` (bon compromis FR/EN).
- Les textes sont concat√©n√©s avec un s√©parateur `eos` avant tokenisation.

### Mod√®le TinyGPT
- Architecture GPT minimale : embeddings, blocs Transformer (MSA + MLP), masque causal, `LayerNorm` final.
- Param√®tres par d√©faut (POC): `d_model=320`, `n_layer=4`, `n_head=4`, `block_size=256`, `dropout=0.1`.

### Entra√Ænement
- Optimiseur : **AdamW**.
- **Gradient clipping** = 1.0.
- √âvaluations p√©riodiques sur `val.bin` (perte moyenne).



---

## üìà Conseils & pistes d‚Äôam√©lioration
- Apprendre un **tokenizer BPE** d√©di√© (lib `tokenizers`) pour de meilleures stats FR.
- Passer √† **RMSNorm**, **poids partag√©s**, ou **FlashAttention** (si CUDA) pour acc√©l√©rer/stabiliser.
- Curriculum learning : commencer avec des s√©quences courtes puis augmenter `block_size`.
- √âvaluer avec perplexit√© + QA de contr√¥le.



---


## Auteur : Arthur PRIGENT

Projet personnel visant √† maitriser les outils de d√©veloppement et de jouer avec les hyper-param√®tres d'un LLM.

